"""
Configura√ß√£o e Execu√ß√£o de Testes End-to-End
PROMPT 4 - Implementa√ß√£o de Testes Automatizados

Sistema de execu√ß√£o e relat√≥rios de todos os testes
"""
import pytest
import sys
import os
import json
import time
from datetime import datetime
from pathlib import Path


class TestRunner:
    """Executor de testes com relat√≥rios detalhados"""
    
    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.results = {}
        
    def run_test_suite(self, test_type="all"):
        """Executa suite de testes espec√≠fica ou completa"""
        self.start_time = time.time()
        
        print("üöÄ INICIANDO SUITE DE TESTES AUTOMATIZADOS")
        print("=" * 60)
        print(f"Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Tipo de teste: {test_type}")
        print("=" * 60)
        
        # Configurar argumentos do pytest
        pytest_args = [
            "-v",  # Verbose
            "--tb=short",  # Traceback curto
            "--strict-markers",  # Markers rigorosos
            "--strict-config",  # Configura√ß√£o rigorosa
        ]
        
        # Adicionar cobertura se dispon√≠vel
        try:
            import pytest_cov
            pytest_args.extend([
                "--cov=app",
                "--cov-report=html:htmlcov",
                "--cov-report=term-missing",
                "--cov-fail-under=70"
            ])
            print("üìä Cobertura de c√≥digo habilitada")
        except ImportError:
            print("‚ö†Ô∏è  pytest-cov n√£o dispon√≠vel, executando sem cobertura")
        
        # Selecionar testes baseado no tipo
        if test_type == "unit":
            pytest_args.extend([
                "tests/test_models.py",
                "tests/test_database.py"
            ])
        elif test_type == "integration":
            pytest_args.extend([
                "tests/test_alerts_api_integration.py"
            ])
        elif test_type == "security":
            pytest_args.extend([
                "tests/test_authentication_security.py"
            ])
        elif test_type == "performance":
            pytest_args.extend([
                "-m", "performance"
            ])
        elif test_type == "all":
            pytest_args.append("tests/")
        else:
            raise ValueError(f"Tipo de teste inv√°lido: {test_type}")
        
        # Executar testes
        print(f"\nüß™ Executando testes: {' '.join(pytest_args)}")
        print("-" * 40)
        
        exit_code = pytest.main(pytest_args)
        
        self.end_time = time.time()
        execution_time = self.end_time - self.start_time
        
        # Processar resultados
        self._process_results(exit_code, execution_time, test_type)
        
        return exit_code == 0
    
    def _process_results(self, exit_code, execution_time, test_type):
        """Processa e exibe resultados dos testes"""
        status = "‚úÖ SUCESSO" if exit_code == 0 else "‚ùå FALHA"
        
        print("\n" + "=" * 60)
        print("üìã RELAT√ìRIO FINAL DOS TESTES")
        print("=" * 60)
        print(f"Status: {status}")
        print(f"C√≥digo de sa√≠da: {exit_code}")
        print(f"Tempo de execu√ß√£o: {execution_time:.2f}s")
        print(f"Tipo de teste: {test_type}")
        
        # Salvar relat√≥rio JSON
        report = {
            'timestamp': datetime.now().isoformat(),
            'test_type': test_type,
            'exit_code': exit_code,
            'success': exit_code == 0,
            'execution_time': execution_time,
            'status': 'PASSED' if exit_code == 0 else 'FAILED'
        }
        
        report_file = f"test_report_{test_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"üìÑ Relat√≥rio salvo em: {report_file}")
        
        # Verificar se h√° arquivo de cobertura
        coverage_file = Path("htmlcov/index.html")
        if coverage_file.exists():
            print(f"üìä Relat√≥rio de cobertura: {coverage_file.absolute()}")


def setup_test_environment():
    """Configura ambiente de teste"""
    print("üîß Configurando ambiente de teste...")
    
    # Definir vari√°veis de ambiente para testes
    os.environ['FLASK_ENV'] = 'testing'
    os.environ['TESTING'] = 'True'
    os.environ['DATABASE_URL'] = 'sqlite:///:memory:'
    
    # Verificar se diret√≥rio de testes existe
    tests_dir = Path("tests")
    if not tests_dir.exists():
        print("‚ùå Diret√≥rio 'tests' n√£o encontrado!")
        return False
    
    # Verificar arquivos de teste necess√°rios
    required_files = [
        "tests/conftest.py",
        "tests/test_models.py",
        "tests/test_database.py",
        "tests/test_alerts_api_integration.py",
        "tests/test_authentication_security.py"
    ]
    
    missing_files = []
    for file_path in required_files:
        if not Path(file_path).exists():
            missing_files.append(file_path)
    
    if missing_files:
        print("‚ùå Arquivos de teste faltando:")
        for file_path in missing_files:
            print(f"   - {file_path}")
        return False
    
    print("‚úÖ Ambiente de teste configurado com sucesso")
    return True


def run_pre_test_validation():
    """Executa valida√ß√µes antes dos testes"""
    print("üîç Executando valida√ß√µes pr√©-teste...")
    
    # Verificar imports cr√≠ticos
    try:
        import flask
        import sqlalchemy
        import pytest
        print("‚úÖ Depend√™ncias principais OK")
    except ImportError as e:
        print(f"‚ùå Depend√™ncia faltando: {e}")
        return False
    
    # Verificar estrutura do projeto
    required_dirs = ["app", "app/models", "app/controllers"]
    
    for dir_path in required_dirs:
        if not Path(dir_path).exists():
            print(f"‚ùå Diret√≥rio faltando: {dir_path}")
            return False
    
    print("‚úÖ Estrutura do projeto OK")
    
    # Verificar arquivos cr√≠ticos
    critical_files = [
        "app/__init__.py",
        "app/models/alerts.py",
        "app/models/user.py",
        "config.py"
    ]
    
    for file_path in critical_files:
        if not Path(file_path).exists():
            print(f"‚ùå Arquivo cr√≠tico faltando: {file_path}")
            return False
    
    print("‚úÖ Arquivos cr√≠ticos OK")
    return True


def main():
    """Fun√ß√£o principal do executor de testes"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Executor de Testes Automatizados - PROMPT 4')
    parser.add_argument(
        '--type', 
        choices=['all', 'unit', 'integration', 'security', 'performance'],
        default='all',
        help='Tipo de teste a executar'
    )
    parser.add_argument(
        '--skip-validation',
        action='store_true',
        help='Pular valida√ß√µes pr√©-teste'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Sa√≠da verbosa'
    )
    
    args = parser.parse_args()
    
    # Banner inicial
    print("\n" + "üß™" * 20)
    print("  SISTEMA DE TESTES AUTOMATIZADOS")
    print("     PROMPT 4 - Sprint 2")
    print("üß™" * 20 + "\n")
    
    # Configurar ambiente
    if not setup_test_environment():
        print("‚ùå Falha na configura√ß√£o do ambiente")
        sys.exit(1)
    
    # Valida√ß√µes pr√©-teste
    if not args.skip_validation:
        if not run_pre_test_validation():
            print("‚ùå Falha nas valida√ß√µes pr√©-teste")
            sys.exit(1)
    
    # Executar testes
    runner = TestRunner()
    success = runner.run_test_suite(args.type)
    
    # Exit code baseado no resultado
    exit_code = 0 if success else 1
    
    print(f"\nüèÅ Execu√ß√£o finalizada com c√≥digo: {exit_code}")
    sys.exit(exit_code)


if __name__ == "__main__":
    main()
